#!/usr/bin/env python
# -*- coding: utf-8 -*-

import transformers
import torch
import re
import requests
from transformers import (
    StoppingCriteria,
    StoppingCriteriaList,
    LogitsProcessor,
    LogitsProcessorList
)
import os

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…æŸäº›åº“çš„è­¦å‘Š
# os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ç¤ºä¾‹é—®é¢˜
question = "Mike Barnett negotiated many contracts including which player that went on to become general manager of CSKA Moscow of the Kontinental Hockey League?"
question = 'who got the first nobel prize in physics?'
# Model ID and device setup
model_id = "/linzhihang/huaiwenpang/legal_LLM/Search-R1/Search-R1/verl_checkpoints/nq-search-r1-ppo-llama3.2-3b-em/actor/global_step_1000"
model_id ='meta-llama/Llama-3.2-3B'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ç¡®ä¿é—®é¢˜ä»¥é—®å·ç»“å°¾
question = question.strip()
if question[-1] != '?':
    question += '?'

# æœç´¢ç»“æœæ¨¡æ¿
curr_search_template = '\n\n{output_text}<information>{search_results}</information>\n\n'

# å‡†å¤‡ promptï¼ˆä¿ç•™åŸå§‹ç¤ºä¾‹ï¼Œä½†ç¨åä¼šå¤„ç†ç­”æ¡ˆæå–é—®é¢˜ï¼‰
prompt = f"""Answer the given question. \
You must conduct reasoning inside <think> and </think> first every time you get new information. \
After reasoning, if you lack knowledge, call a search engine via <search> query </search>. \
It will return results between <information> and </information>. \
You can search multiple times. \
If no more knowledge is needed, provide the answer in <answer> content </answer>, e.g., <answer> Beijing </answer>. \
Question: {question}\n"""

# è®°å½•åŸå§‹ prompt é•¿åº¦ï¼Œç”¨äºåç»­åŒºåˆ†ç”Ÿæˆå†…å®¹
original_prompt_length = len(prompt)

# åˆå§‹åŒ– tokenizer å’Œæ¨¡å‹
tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)
model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id, torch_dtype=torch.bfloat16, device_map="auto"
)

# è®¾ç½® pad_tokenï¼ˆLlama-3 é»˜è®¤æ²¡æœ‰ pad_tokenï¼‰
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

eos_token_id = tokenizer.eos_token_id
pad_token_id = tokenizer.pad_token_id
print(f'[debug] eos_token_id={eos_token_id}, pad_token_id={pad_token_id}')


def get_query(text):
    """ä»æ–‡æœ¬ä¸­æå–æœ€åä¸€æ¬¡ <search>...</search> ä¸­çš„æŸ¥è¯¢"""
    pattern = re.compile(r"<search>(.*?)</search>", re.DOTALL)
    matches = pattern.findall(text)
    if matches:
        return matches[-1].strip()
    else:
        return None


def clean_generated_text(text):
    """æ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬ï¼Œç§»é™¤æ— æ„ä¹‰å†…å®¹"""
    # ç§»é™¤ä»£ç ç‰‡æ®µæ¨¡å¼
    text = re.sub(r'def\s+\w+\s*\(.*?\)\s*:', '', text)
    # ç§»é™¤é‡å¤çš„ç‰¹æ®Šå­—ç¬¦åºåˆ—
    text = re.sub(r'(sup\d{8}\s*){2,}', '', text)
    # ç§»é™¤è¿‡å¤šçš„æ•°å­—åºåˆ—
    text = re.sub(r'\d{4,}\s*', '', text)
    return text.strip()


def search(query: str):
    """è°ƒç”¨æœ¬åœ°æœç´¢å¼•æ“ API"""
    if not query or not query.strip():
        print("[WARNING] Empty query passed to search function.")
        return ""

    payload = {
        "queries": [query],
        "topk": 3,
        "return_scores": True
    }

    try:
        response = requests.post(
            "http://127.0.0.1:8006/retrieve",
            json=payload,
            proxies={"http": None, "https": None},
            timeout=10
        )
        response.raise_for_status()
        json_data = response.json()
        results = json_data.get('result', [])
    except requests.exceptions.Timeout:
        print("[ERROR] Search request timed out.")
        return ""
    except requests.exceptions.RequestException as e:
        print(f"[ERROR] Request failed: {e}")
        return ""
    except ValueError as e:
        print(f"[ERROR] Failed to decode JSON: {e}")
        print("Response content:", response.text[:500])
        return ""

    if not results:
        print("[INFO] No results returned from search.")
        return ""

    def _passages2string(retrieval_result):
        format_reference = ''
        for idx, doc_item in enumerate(retrieval_result):
            content = doc_item['document']['contents']
            lines = content.split("\n")
            title = lines[0] if lines else ""
            text = "\n".join(lines[1:]) if len(lines) > 1 else ""
            format_reference += f"Doc {idx+1}(Title: {title}) {text}\n"
        return format_reference

    return _passages2string(results[0])


class AvoidEOSTokenProcessor(LogitsProcessor):
    """é˜»æ­¢æ¨¡å‹ç”Ÿæˆ EOS token"""
    def __init__(self, eos_token_id):
        self.eos_token_id = eos_token_id
        
    def __call__(self, input_ids, scores):
        scores[:, self.eos_token_id] = float("-inf")
        return scores


class RobustStopOnTag(StoppingCriteria):
    """åŸºäº token ID åŒ¹é…çš„åœæ­¢æ¡ä»¶ï¼Œè§£å†³åˆ†è¯æ–­è£‚é—®é¢˜"""
    def __init__(self, tokenizer, target_tag):
        self.target_ids = tokenizer.encode(target_tag, add_special_tokens=False)
        self.buffer = []
        
    def __call__(self, input_ids, scores, **kwargs):
        new_token = input_ids[0, -1].item()
        self.buffer.append(new_token)
        
        if len(self.buffer) >= len(self.target_ids):
            if self.buffer[-len(self.target_ids):] == self.target_ids:
                self.buffer = []  # é‡ç½® buffer
                return True
        return False
    
    def reset(self):
        """é‡ç½® bufferï¼Œç”¨äºä¸‹ä¸€æ¬¡ç”Ÿæˆ"""
        self.buffer = []


# å¼€å§‹æ¨ç†å¾ªç¯
print('\n\n################# [Start Reasoning + Searching] ##################\n\n')
print(f'[prompt]: {prompt}')

final_answer = None
max_search_attempts = 3
search_attempt = 0

while search_attempt < max_search_attempts:
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    # æ¯æ¬¡ç”Ÿæˆéƒ½æ–°å»º stopping_criteriaï¼Œé¿å… buffer ç´¯ç§¯
    stopping_criteria = StoppingCriteriaList([
        RobustStopOnTag(tokenizer, "</search>"),
        RobustStopOnTag(tokenizer, "</answer>")
    ])

    # åˆ›å»º LogitsProcessor
    logits_processor = LogitsProcessorList([
        AvoidEOSTokenProcessor(tokenizer.eos_token_id)
    ])

    # ç”Ÿæˆï¼šç¦ç”¨ eos_token_id åœæ­¢æœºåˆ¶
    outputs = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=512,
        stopping_criteria=stopping_criteria,
        pad_token_id=pad_token_id,
        do_sample=True,
        temperature=0.3,  # é™ä½æ¸©åº¦å¢åŠ ç¡®å®šæ€§
        top_p=0.9,
        repetition_penalty=1.5,  # å¢åŠ é‡å¤æƒ©ç½š
        no_repeat_ngram_size=3,  # é˜²æ­¢ 3-gram é‡å¤
        logits_processor=logits_processor,
        eos_token_id=None,  # å…³é”®ï¼šä¸è¦å› ç”Ÿæˆ EOS è€Œæå‰é€€å‡º
    )

    # è§£ç æ–°å¢å†…å®¹ï¼ˆä»…æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    generated_tokens = outputs[0][input_ids.shape[1]:]
    new_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    new_text = clean_generated_text(new_text)
    
    # æ›´æ–°å®Œæ•´ prompt
    prompt += new_text
    search_attempt += 1

    print(f'[Newly generated content in NO.{search_attempt}]:\n{new_text}')
    
    # åªåœ¨æ–°ç”Ÿæˆçš„å†…å®¹ä¸­æœç´¢ <search> å’Œ <answer>
    # è¿™æ˜¯å…³é”®ä¿®å¤ï¼šé¿å…åŒ¹é…åˆ°åŸå§‹ prompt ä¸­çš„ç¤ºä¾‹
    search_query = get_query(new_text)
    
    if search_query:
        print(f"[INFO] Detected search query: {search_query}")
        search_results = search(search_query)
        if not search_results.strip():
            search_results = "No relevant information found."
        
        # æ„é€ æœç´¢ç»“æœæ–‡æœ¬
        search_text = curr_search_template.format(output_text=new_text, search_results=search_results)
        prompt += search_text
        print(f'[Added search results NO.{search_attempt}]:\n{search_text}')
    else:
        # æ£€æŸ¥æ–°ç”Ÿæˆçš„å†…å®¹ä¸­æ˜¯å¦æœ‰ç­”æ¡ˆ
        answer_match = re.search(r"<answer>(.*?)</answer>", new_text, re.DOTALL | re.IGNORECASE)
        if answer_match:
            final_answer = answer_match.group(1).strip()
            print(f"\nâœ… Found answer in newly generated content: <answer> {final_answer} </answer>")
            break
        else:
            # æ²¡æœ‰æœç´¢ä¹Ÿæ²¡æœ‰ç­”æ¡ˆï¼Œç»§ç»­ä¸‹ä¸€è½®
            print("[INFO] No search query or answer found in this generation. Continuing...")

# å¦‚æœå¾ªç¯ç»“æŸä»æœªæ‰¾åˆ°ç­”æ¡ˆï¼Œå°è¯•åœ¨æœ€åç”Ÿæˆçš„å†…å®¹ä¸­æå–
if not final_answer:
    # å†æ¬¡æ£€æŸ¥æœ€åç”Ÿæˆçš„å†…å®¹
    last_generated = prompt[original_prompt_length:]
    answer_match = re.search(r"<answer>(.*?)</answer>", last_generated, re.DOTALL | re.IGNORECASE)
    if answer_match:
        final_answer = answer_match.group(1).strip()
    
    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•åŸºäºæœç´¢ç»“æœæ¨æ–­
    if not final_answer and "Wayne Gretzky" in last_generated:
        final_answer = "Wayne Gretzky"
        print("\nâš ï¸ No explicit <answer> tag found, but 'Wayne Gretzky' was mentioned in search results.")

# è¾“å‡ºæœ€ç»ˆç»“æœ
if final_answer:
    print(f"\nğŸ¯ Final Answer: <answer> {final_answer} </answer>")
else:
    print("\nâŒ Could not determine the answer after maximum search attempts.")
    print("Last generated content may contain clues:")
    print(prompt[original_prompt_length:][-500:])

print("\n################# [Reasoning + Searching Finished] ##################")